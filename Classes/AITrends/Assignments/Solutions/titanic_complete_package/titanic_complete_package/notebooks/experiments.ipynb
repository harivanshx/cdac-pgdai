{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö¢ Titanic Machine Learning ‚Äî End-to-End Experiment\n",
    "\n",
    "This notebook walks you through an end-to-end binary classification workflow:\n",
    "\n",
    "1. Load & clean the Titanic dataset  \n",
    "2. Train multiple models: Logistic, SVM, Decision Tree, Random Forest, AdaBoost  \n",
    "3. Evaluate with ROC and Precision-Recall curves  \n",
    "4. Compute **Accuracy**, **Sensitivity**, **Specificity**, **Precision**, **F1** across thresholds  \n",
    "5. Find best threshold and evaluate on test set\n",
    "\n",
    "**Dataset:** Download `train.csv` from [Kaggle Titanic competition](https://www.kaggle.com/competitions/titanic/data) and place it in `../data/train.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, f1_score, confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "import joblib, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/train.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df[['Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Preprocessor & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['Pclass','Age','SibSp','Parch','Fare']\n",
    "categorical = ['Sex','Embarked']\n",
    "pre = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical)\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'Logistic': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=200, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, m in models.items():\n",
    "    pipe = Pipeline([('pre', pre), ('model', m)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    probs = pipe.predict_proba(X_val)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_val, probs)\n",
    "    pr_prec, pr_rec, _ = precision_recall_curve(y_val, probs)\n",
    "    results[name] = {'pipe': pipe, 'fpr': fpr, 'tpr': tpr, 'pr_prec': pr_prec, 'pr_rec': pr_rec,\n",
    "                     'roc_auc': auc(fpr, tpr), 'pr_auc': auc(pr_rec, pr_prec)}\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "for n,r in results.items():\n",
    "    plt.plot(r['fpr'], r['tpr'], label=f\"{n} (AUC={r['roc_auc']:.2f})\")\n",
    "plt.plot([0,1],[0,1],'--',c='gray'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.title('ROC Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Metrics vs Threshold (for best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = max(results, key=lambda n: results[n]['roc_auc'])\n",
    "pipe = results[best_name]['pipe']\n",
    "probs = pipe.predict_proba(X_val)[:,1]\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "\n",
    "accs, sens, specs, f1s, precs = [], [], [], [], []\n",
    "for t in thresholds:\n",
    "    preds = (probs >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, preds).ravel()\n",
    "    accs.append((tp+tn)/(tp+tn+fp+fn))\n",
    "    sens.append(tp/(tp+fn))\n",
    "    specs.append(tn/(tn+fp))\n",
    "    f1s.append(f1_score(y_val, preds))\n",
    "    precs.append(precision_score(y_val, preds))\n",
    "\n",
    "best_t = thresholds[np.argmax(f1s)]\n",
    "print(f\"Best threshold = {best_t:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, accs, label='Accuracy')\n",
    "plt.plot(thresholds, sens, label='Sensitivity')\n",
    "plt.plot(thresholds, specs, label='Specificity')\n",
    "plt.plot(thresholds, f1s, label='F1')\n",
    "plt.legend(); plt.xlabel('Threshold'); plt.ylabel('Metric'); plt.title(f'{best_name} - Metrics vs Threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Final test evaluation using best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_test = pipe.predict_proba(X_test)[:,1]\n",
    "preds = (probs_test >= best_t).astype(int)\n",
    "print('Accuracy:', accuracy_score(y_test, preds))\n",
    "print('Precision:', precision_score(y_test, preds))\n",
    "print('Recall:', recall_score(y_test, preds))\n",
    "print('F1:', f1_score(y_test, preds))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(pipe, f'../models/model_{best_name.lower()}.pkl')\n",
    "print(f'Model saved as model_{best_name.lower()}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
